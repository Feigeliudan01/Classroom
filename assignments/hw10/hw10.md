# Homework 10: Getting data from the Web

## Overview

The homework is due December 06, 2018, at 23:59. This homework is *Optional*, meaning that your homework grade for STAT 547 will be composed of the top 4 grades received in your homework submissions. The same is true for peer reviews: we'll take the top four.

Your task, high level: do scraping twice, and two API requests. 

It would probably make the most sense to do the bulk of your work in an `.Rmd` file or two, but there are other methods that make sense.

### Getting Started

As usual, create a homework repository by following the link provided on canvas. 

### Evaluation

All [rubrics](http://stat545.com/Classroom/assignments/#evaluation) __except__ the "graphs" and "tables" rubrics will be used to evaluate this assignment.

The scraping and API parts are weighted equally.

Here are some ideas to add "wow" to your submission:

- Interact with an API more than just with `GET`. Maybe interact with your homework repo through the GitHub API (such as pushing and pulling). Maybe create an Issue through the GitHub API. The use of the `gh` package for this is totally acceptable.
- Use the `RCurl` package instead of / in addition to `httr`. Or, maybe even compare various methods of obtaining data (even between scraping and using an API on the same data). Would be really cool to see a "tutorial style" submission highlighting the differences between various methods.
- Make something like Andrew MacDonald's [gameday function](http://stat545.com/webdata01_slides.html#16), which upon using, gives you some real-time information about something. Maybe the weather? The news?
	- Note that the `gameday` function doesn't seem to be working. The nhle website probably changed since its creation.

Keep in mind that doing things unrelated to the topic, although possibly wow'ing a reader, would not be worth additional marks. For example, doing a sentiment analysis on data you retrieve from Twitter would be great, but not worth anything extra in the context of this assignment.

### Submitting

Once you're done the assignment, go back to UBC canvas, and find the "Homework 10" page. Here, do the following:

- Provide a link to your homework repository.
- Write a brief reflection about your experience with this assignment: what was hard/easy, problems you solved, helpful tutorials you read, etc. No need to write lots here, this just allows us to make improvements for next year.

Although you don't have to do this until the very end, we highly recommend you commit and push to your homework repo regularly!

## The Assignment

### Scrape data

Use the `rvest` package to scrape data from the web to make two data objects.

Requirements:

- At least one of your data objects should be a data frame that contains at least 2 rows and 2 columns, and this should not be obtained from activities done in class.
- You should do some CSS selection (use `html_nodes()` or `html_node()`) in at least one case.

### Make API queries

Make two requests for data two make two data objects. 

Requirements:

- At least one of your data objects should be a data frame that contains at least 2 rows and 2 columns, and this should not be obtained from the GitHub API or the OMDb API that we used in class.
- Use the `httr` package to do the retrieval (or even the `RCurl` package if you'd like).
- Don't use R packages that are specifically designed to wrap a specific API (such as `rebird` or `geonames`, as listed in the cm112 notes). 
	- You _can_ use these, but not until you've completed the original task.
	- You _can_ use the API's associated with these, though.

## Notes

### Privacy and security

Remember to protect any API keys or tokens, if your chosen API requires them! Don't commit and push those to GitHub. It might be best to leave a space early your script with a line like this: 

```
my_api_key <- # Put API key here
```

Or perhaps have it read from a plain text file that you've `.gitignore`d.

Also, make sure you not violating a site's terms of service or your own ethical standards with your webscraping. Just because you can, it doesn't mean you should!

### Output

- The data that you read in, in all cases, should be in clean and tidy format.
- __Store each data object as a file__ (eg. `csv`, `tsv`, `txt`) -- you should end up with four.
- In your report(s), do just enough basic exploration of the resulting data, possibly including some plots, that you and a reader are convinced you've successfully downloaded and cleaned it, and can get a good sense of what the data contains (without printing it to screen, unless it's small, which is OK too).
- If there are many steps involved in wrangling the data, it might be useful to make it clear what the data looks like after it has just been read in, so that the downstream wrangling might be clearer.
- The data you retrieve should make sense with regards to a hypothetical analysis, or use-case. In other words, there should be some thought put into the type of data you retrieve, as opposed to just retrieving random nonsense.

