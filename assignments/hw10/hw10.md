# Homework 10: Getting data from the Web

## Overview

The homework is due December 06, 2018, at 23:59. This homework is *Optional*, meaning that your homework grade for STAT 547 will be composed of the top 4 grades received in your homework submissions. The same is true for peer reviews: we'll take the top four.

Your task, high level: do scraping twice, and two API requests, to end up with four data frames total, which you should export (to `csv` or `tsv`). It would probably make the most sense to do this in an `.Rmd` file or two.

### Getting Started

As usual, create a homework repository by following the link provided on canvas. 

### Evaluation

All [rubrics](http://stat545.com/Classroom/assignments/#evaluation) __except__ the "graphs" and "tables" rubrics will be used to evaluate this assignment.

The scraping and API parts are weighted equally.

Here are some ideas to add "wow" to your submission:

- Interact with an API more than just with `GET`. Maybe interact with your homework repo through the GitHub API (such as pushing and pulling). Maybe create an Issue through the GitHub API. The use of the `gh` package for this is totally acceptable.
- Use the `RCurl` package instead of / in addition to `httr`. Or, maybe even compare various methods of obtaining data (even between scraping and using an API on the same data). Would be really cool to see a "tutorial style" submission highlighting the differences between various methods.
- Make something like Andrew MacDonald's [gameday function](http://stat545.com/webdata01_slides.html#16), which upon using, gives you some real-time information about something. Maybe the weather? The news?
	- Note that the `gameday` function doesn't seem to be working. The nhle website probably changed since its creation.

Keep in mind that doing things unrelated to the topic, although possibly wow'ing a reader, would not be worth additional marks. For example, doing a sentiment analysis on data you retrieve from Twitter would be great, but not worth anything extra in the context of this assignment.

### Submitting

Once you're done the assignment, go back to UBC canvas, and find the "Homework 10" page. Here, do the following:

- Provide a link to your homework repository.
- Write a brief reflection about your experience with this assignment: what was hard/easy, problems you solved, helpful tutorials you read, etc. No need to write lots here, this just allows us to make improvements for next year.

Although you don't have to do this until the very end, we highly recommend you commit and push to your homework repo regularly!

## The Assignment

### Scrape data

Use the `rvest` package to scrape data from the web! 

Requirements:

- You should end up with a data frame / tibble with at least two columns and two rows.
- You should do some CSS selection (use `html_nodes()` or `html_node()`).
- The table should make sense with regards to a hypothetical analysis. In other words, there should be some thought put into the type of data you retrieve, as opposed to just putting random nonsense into the tibble's cells.


### Make API queries

Choose an API that is different from the GitHub API and the OMDb API we saw in class (for inspiration, see near the end of the cm112 notes, "API wrappers and examples"). Make two requests for data, and bring each request in to a data frame that's "ready for" a hypothetical analysis (which you don't have to describe). 

- Use the `httr` package to do the retrieval (or even the `RCurl` package if you'd like).
- Don't use R packages that are specifically designed to wrap a specific API (such as `rebird` or `geonames`, as listed in the cm112 notes). 
	- You _can_ use these, but not until you've completed the original task.
	- You _can_ use the API's associated with these, though.

## Notes

### Privacy and security

Remember to protect any API keys or tokens, if your chosen API requires them! Don't commit and push those to GitHub. It might be best to leave a space early your script with a line like this: 

```
my_api_key <- # Put API key here
```

Or perhaps have it read from a plain text file that you've `.gitignore`d.

Also, make sure you not violating a site's terms of service or your own ethical standards with your webscraping. Just because you can, it doesn't mean you should!

### Output

- The data that you read in, in all cases, should be in clean and tidy data frames. 
- __Store your data frames as a file__ (eg. `csv` or `tsv`) ready for (hypothetical!) downstream analysis. 
- In your report(s), do just enough basic exploration of the resulting data, possibly including some plots, that you and a reader are convinced you've successfully downloaded and cleaned it, and can get a good sense of what the data frame contains (without printing it to screen, unless it's small).
- If there are many steps involved in wrangling the data, it might be useful to make it clear what the data looks like after it has just been read in, so that the downstream wrangling might be clearer.